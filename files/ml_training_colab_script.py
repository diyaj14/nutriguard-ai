# -*- coding: utf-8 -*-
"""colab_training_script.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1...

# AI Personalization Engine Development
This notebook handles:
1.  Synthetic Data Generation (Products & Users)
2.  Ground Truth Labeling (Suitability Score Calculation based on Rules)
3.  Model Training (Gradient Boosting Regressor)
4.  Evaluation
"""

# Dependencies - Run in Colab cell:
# !pip install pandas numpy scikit-learn matplotlib seaborn

import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ==========================================
# 1. SYNTHETIC DATA GENERATION
# ==========================================

def generate_synthetic_products(n=1000):
    products = []
    categories = ['Snack', 'Beverage', 'Dairy', 'Cereal', 'Ready-Meal', 'Condiment']
    
    for _ in range(n):
        category = random.choice(categories)
        
        # Basic nutritional profiles based on category heuristics
        if category == 'Snack':
            sugar = np.random.uniform(10, 60)
            fat = np.random.uniform(10, 40)
            salt = np.random.uniform(0.5, 3.0)
            fiber = np.random.uniform(0, 5)
        elif category == 'Beverage':
            sugar = np.random.uniform(0, 15)
            fat = np.random.uniform(0, 5)
            salt = np.random.uniform(0, 0.2)
            fiber = np.random.uniform(0, 1)
        elif category == 'Cereal':
            sugar = np.random.uniform(5, 30)
            fat = np.random.uniform(2, 15)
            salt = np.random.uniform(0.5, 1.5)
            fiber = np.random.uniform(2, 10)
        else: # Generic
            sugar = np.random.uniform(0, 50)
            fat = np.random.uniform(0, 30)
            salt = np.random.uniform(0, 2.0)
            fiber = np.random.uniform(0, 5)
            
        p = {
            'product_id': f"P{_}",
            'category': category,
            'energy_kcal_100g': (4 * sugar) + (9 * fat) + (4 * np.random.uniform(0, 20)), # Approx
            'sugar_100g': sugar,
            'fat_100g': fat,
            'saturated_fat_100g': fat * np.random.uniform(0.1, 0.6),
            'salt_100g': salt,
            'fiber_100g': fiber,
            'protein_100g': np.random.uniform(0, 25),
            'nova_group': np.random.choice([1, 2, 3, 4], p=[0.1, 0.2, 0.3, 0.4]),
            'contains_gluten': np.random.choice([0, 1], p=[0.8, 0.2]),
            'contains_peanut': np.random.choice([0, 1], p=[0.95, 0.05]),
            'contains_milk': np.random.choice([0, 1], p=[0.7, 0.3]),
            'contains_egg': np.random.choice([0, 1], p=[0.9, 0.1])
        }
        products.append(p)
    return pd.DataFrame(products)

def generate_synthetic_users(n=1000):
    users = []
    for _ in range(n):
        u = {
            'user_id': f"U{_}",
            'has_hypertension': np.random.choice([0, 1], p=[0.7, 0.3]),
            'has_diabetes': np.random.choice([0, 1], p=[0.85, 0.15]),
            'has_high_cholesterol': np.random.choice([0, 1], p=[0.8, 0.2]),
            'gluten_intolerance': np.random.choice([0, 1], p=[0.9, 0.1]),
            'peanut_allergy': np.random.choice([0, 1], p=[0.98, 0.02]),
            'lactose_intolerance': np.random.choice([0, 1], p=[0.8, 0.2]),
            'egg_allergy': np.random.choice([0, 1], p=[0.97, 0.03]),
            'goal_weight_loss': np.random.choice([0, 1], p=[0.5, 0.5]),
            'goal_muscle_gain': np.random.choice([0, 1], p=[0.7, 0.3])
        }
        users.append(u)
    return pd.DataFrame(users)

# ==========================================
# 2. SUITABILITY SCORE CALCULATION (LABELING)
# ==========================================

def calculate_suitability(row):
    """
    Calculates the detailed Suitability Score (S) based on Product (P) and User (U) features.
    S = 100 - Penalties
    Subject to Hard Constraints (S=0)
    """
    score = 100
    p = row # Product features
    u = row # User features (merged in row)
    
    # --- HARD CONSTRAINTS (Safety) ---
    if u['peanut_allergy'] and p['contains_peanut']: return 0
    if u['gluten_intolerance'] and p['contains_gluten']: return 0
    if u['lactose_intolerance'] and p['contains_milk']: return 0
    if u['egg_allergy'] and p['contains_egg']: return 0
    
    # --- SOFT PENALTIES (Health Conditions) ---
    
    # Hypertension: Penalize high salt
    if u['has_hypertension']:
        if p['salt_100g'] > 1.5: score -= 40
        elif p['salt_100g'] > 0.5: score -= 20
        
    # Diabetes: Penalize high sugar & high index carbs (using sugar proxy)
    if u['has_diabetes']:
        if p['sugar_100g'] > 22.5: score -= 50
        elif p['sugar_100g'] > 5.0: score -= 25
        
    # High Cholesterol: Penalize Saturated Fat
    if u['has_high_cholesterol']:
        if p['saturated_fat_100g'] > 5.0: score -= 30
        
    # --- GOALS (Fitness) ---
    
    # Weight Loss: Penalize high calories & sugar
    if u['goal_weight_loss']:
        if p['energy_kcal_100g'] > 400: score -= 20
        if p['sugar_100g'] > 15: score -= 15
        
    # Muscle Gain: Reward Protein
    if u['goal_muscle_gain']:
        if p['protein_100g'] > 20: score += 10 # Bonus
        elif p['protein_100g'] < 5: score -= 10
        
    # --- GENERAL HEALTH (Base Penalties for everyone) ---
    # NOVA Group Penalties
    if p['nova_group'] == 4: score -= 15
    elif p['nova_group'] == 3: score -= 5
    
    # Cap score at 100 and floor at 0
    return max(0, min(100, score))

# Generate Dataset
print("Generating Synthetic Data...")
products_df = generate_synthetic_products(500)
users_df = generate_synthetic_users(500)

# Cross Join to create all User-Product interactions for training
# We take a sample to keep it manageable
print("Creating Interaction Matrix...")
import itertools
interaction_data = []

# Only take smaller interaction set for demo speed
sample_products = products_df.sample(200) 
sample_users = users_df.sample(200)

# Cartesian product simulation
cross_join = sample_users.assign(key=1).merge(sample_products.assign(key=1), on='key').drop('key', axis=1)

print(f"Dataset Size: {len(cross_join)} interactions")

# Calculate Ground Truth Labels
print("Calculating Ground Truth Scores...")
cross_join['suitability_score'] = cross_join.apply(calculate_suitability, axis=1)

# ==========================================
# 3. MODEL TRAINING
# ==========================================

# Feature Selection
X = cross_join.drop(['user_id', 'product_id', 'suitability_score'], axis=1)
y = cross_join['suitability_score']

# Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing Pipeline
categorical_features = ['category']
numerical_features = [col for col in X.columns if col not in categorical_features]

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Model Definition
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42))
])

print("\nTraining Model...")
model.fit(X_train, y_train)

# ==========================================
# 4. EVALUATION
# ==========================================

print("\nEvaluating Model...")
y_pred = model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R^2 Score: {r2:.2f}")

# ==========================================
# 5. EXPORT MODEL FOR LOCAL USE
# ==========================================

import joblib

# Save the entire pipeline (Preprocessor + Model)
model_filename = 'personalization_model.pkl'
joblib.dump(model, model_filename)

print(f"\nâœ… Model saved as '{model_filename}'")

# Code to download the file automatically if running in Google Colab
try:
    from google.colab import files
    print("Triggering download to your local machine...")
    files.download(model_filename)
except ImportError:
    print("Not running in Google Colab, model saved locally.")

print("\nIMPORTANT: Move this 'personalization_model.pkl' file to your local 'backend' folder.")
print("You will use it to make predictions on real API data.")

